{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4137655c",
   "metadata": {},
   "source": [
    "1. Exploratory Data Analysis (EDA) \n",
    "Describe the dataset.\n",
    "\n",
    "131647 rows\n",
    "4 columns\n",
    "\n",
    "Columns:\n",
    "taxonomy - 136 unique values\n",
    "page_type - CLP and PLP are different types of pages and advertising on one vs. the other could be meaningful. I'm going to make an assumption that a CLP is more generic than a PLP; for example, Doors & Windows/Exterior Doors is a CLP and Doors & Windows/Exterior Doors/Front Doors is a PLP. If I were in the business of selling front doors, I would really want to advertise on the Doors & Windows/Exterior Doors/Front Doors PLP. However, the data does not specify such detail of a PLP. I'm going to make an executive decision, as far as this analysis goes, I am going to discard page_type and use taxonomy as the fundamental unit of analysis as opposed to taxonomy-page_type being the fundamental unit of analysis. The task is to predict web traffic, and traffic to a given taxonomy is still traffic whether it is a CLP or a PLP. \n",
    "week_start_date - Although this column name has 'week' in it, it appears to be daily and thus a better label would be 'date' or 'traffic_date'\n",
    "page_views - \n",
    "\n",
    "What does each column represent?\n",
    "\n",
    "Are there any missing values or anomalies?\n",
    "- Some arbitrary spikes in volume\n",
    "\n",
    "How do these characteristics impact the analysis?\n",
    "\n",
    "Visualize the data.\n",
    "\n",
    "Provide visualizations that offer insights into the dataset's distribution, trends, and patterns.\n",
    "\n",
    "How do these insights inform your modeling approach?\n",
    "- Many time series patterns, fit a handful of models to each taxonomy and choose best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in case study data\n",
    "os.chdir(\"/Users/jh/Downloads\")\n",
    "tx_df = pd.read_excel(\"Retail Category Forecasting.xlsx\", sheet_name=\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e04208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data\n",
    "print(tx_df.head())\n",
    "print(\"\")\n",
    "\n",
    "# How big is the dataset\n",
    "print(f\"Dataset dimensions: {tx_df.shape}\")\n",
    "print(\"\")\n",
    "\n",
    "# Data types\n",
    "print(\"Column data types:\")\n",
    "print(tx_df.dtypes)\n",
    "print(\"\")\n",
    "\n",
    "# Unique taxonomy count\n",
    "print(f\"Unique taxonomy count: {len(tx_df['taxonomy'].unique())}\")\n",
    "print(\"\")\n",
    "\n",
    "# Statistics of page_views\n",
    "print(\"Statistics of page_views\")\n",
    "print(tx_df['page_views'].describe())\n",
    "print(\"\")\n",
    "\n",
    "# Statistics of week_start_date\n",
    "print(\"Statistics of week_start_date\")\n",
    "print(tx_df['week_start_date'].describe(datetime_is_numeric=True))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View total page views by week_start_date\n",
    "pv_x_wsd = tx_df.groupby('week_start_date')['page_views'].sum().reset_index().sort_values('week_start_date')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title('Total Page Views by Date')\n",
    "plt.plot(pv_x_wsd['week_start_date'], pv_x_wsd['page_views'], linewidth=0.5)\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6120cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"PLP vs. CLP record counts:\")\n",
    "print(tx_df['page_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82f0aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View total page views by week_start_date and page_type\n",
    "pv_x_wsd_pt = tx_df.groupby(['week_start_date', 'page_type'])['page_views'].sum().reset_index().sort_values(['page_type', 'week_start_date'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title('Page Views by Date and CLP/PLP')\n",
    "plt.plot(pv_x_wsd_pt[pv_x_wsd_pt['page_type'] == 'PLP']['week_start_date'], pv_x_wsd_pt[pv_x_wsd_pt['page_type'] == 'PLP']['page_views'], linewidth=0.5, color=\"blue\")\n",
    "plt.plot(pv_x_wsd_pt[pv_x_wsd_pt['page_type'] == 'CLP']['week_start_date'], pv_x_wsd_pt[pv_x_wsd_pt['page_type'] == 'CLP']['page_views'], linewidth=0.5, color=\"red\")\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.legend(['PLP', 'CLP'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if there are any missing dates\n",
    "# Get DF with unique week_start_date and lag 1 of week_start_date\n",
    "wsd = pd.concat([pd.DataFrame(tx_df['week_start_date'].unique(), columns=['week_start_date']),\n",
    "                 pd.DataFrame(tx_df['week_start_date'].unique(), columns=['week_start_date_lag']).shift(periods=1)],\n",
    "                axis=1\n",
    "               )\n",
    "\n",
    "# Get days difference between each pair of week_start_date and week_start_date_lag\n",
    "wsd['days_diff'] = wsd['week_start_date'] - wsd['week_start_date_lag']\n",
    "\n",
    "# Look at the distribution of 'days_diff'\n",
    "# If all values are 1, then no missing days, if any values are greater than 1, then there are missing days\n",
    "print(wsd['days_diff'].value_counts())\n",
    "# Days diff are all 1 days, the data as a whole has no missing days\n",
    "\n",
    "# The label week_start_date does not make sense with daily observations\n",
    "# A better label could be 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f200fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_wsd_df = tx_df[['taxonomy', 'week_start_date']].drop_duplicates()['taxonomy'].value_counts().reset_index()\n",
    "tx_wsd_df.columns = [\"taxonomy\", \"cnt_of_days\"]\n",
    "print(tx_wsd_df.sort_values('cnt_of_days').iloc[:20, :])\n",
    "print(tx_wsd_df[\"cnt_of_days\"].value_counts().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9568727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore CLP vs. PLP\n",
    "# How many taxonomies have both CLP and PLP page types\n",
    "print(tx_df[['taxonomy', 'page_type']].drop_duplicates().groupby('taxonomy').count().sort_values('page_type', ascending=False))\n",
    "print(\"\")\n",
    "# 4 taxonomies have CLP and PLP page_types\n",
    "\n",
    "# Isolate taxonomies with 2 page_types\n",
    "multi_pt_tx = tx_df[['taxonomy', 'page_type']].drop_duplicates().groupby('taxonomy').count().reset_index()\n",
    "print(multi_pt_tx[multi_pt_tx['page_type'] > 1]['taxonomy'].to_list())\n",
    "print(\"\")\n",
    "\n",
    "# Check if any of those 4 taxonomies have different page_type views on the same day\n",
    "tmp_df = tx_df[['taxonomy', 'page_type', 'week_start_date']].drop_duplicates().groupby(['taxonomy', 'week_start_date']).count().sort_values('page_type', ascending=False)\n",
    "print(tmp_df['page_type'].value_counts())\n",
    "print(\"\")\n",
    "\n",
    "print(tx_df[['taxonomy', 'page_type']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dcb9621",
   "metadata": {},
   "source": [
    "Executive decision:\n",
    "Treat CLP and PLP page_types the same\n",
    "\n",
    "Rationale:\n",
    "Whether the traffic is to a CLP or a PLP, it is still traffic\n",
    "The task is to predict total web traffic to a taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08610e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF without page_type\n",
    "tx_df2 = tx_df.groupby(['taxonomy', 'week_start_date'])['page_views'].sum().reset_index().rename(columns={'week_start_date': 'traffic_date'})\n",
    "tx_df2.columns\n",
    "print(tx_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at 'flooring>garage flooring' taxonomy and plot\n",
    "# 'flooring>garage flooring' does not have page_views on every date\n",
    "tmp_df = tx_df2[tx_df2['taxonomy'] == 'flooring>garage flooring'].sort_values('traffic_date').reset_index(drop=True)\n",
    "print(tmp_df)\n",
    "\n",
    "# Some days have page_views = 0\n",
    "# This taxonomy has missing dates\n",
    "# Assumption: missing dates have page_views = 0\n",
    "\n",
    "# View total page views by traffic_date\n",
    "plt.figure(figsize=(10,4))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title('Garage Flooring Page Views by Date')\n",
    "plt.plot(tmp_df['traffic_date'], tmp_df['page_views'], linewidth=0.5)\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843549f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many page_views = 0 across all taxonomies\n",
    "pv_zero_cnt = tx_df2[tx_df2['page_views'] == 0].count()[0]\n",
    "pv_gt_zero_cnt = tx_df2[tx_df2['page_views'] > 0].count()[0]\n",
    "\n",
    "print(f\"Count of page_views = 0: {pv_zero_cnt}\")\n",
    "print(f\"Count of page_views > 0: {pv_gt_zero_cnt}\")\n",
    "print(f\"Percent of page_views = 0: {pv_zero_cnt/tx_df2.shape[0]}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1201652d",
   "metadata": {},
   "source": [
    "Ensure that all taxonomies have traffic_dates from first date where page_views > 0 to 1/21/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first traffic_date where page_views > 0 per taxonomy\n",
    "tx_dt1 = tx_df2[tx_df2['page_views'] > 0].groupby('taxonomy')['traffic_date'].min().reset_index().rename(columns={\"traffic_date\": \"first_traffic_date\"})\n",
    "\n",
    "# Get all unique taxonomies and all unique traffic_dates\n",
    "tx = pd.DataFrame(tx_df2['taxonomy'].drop_duplicates().reset_index(drop=True))\n",
    "dts = pd.DataFrame(tx_df2['traffic_date'].drop_duplicates().reset_index(drop=True))\n",
    "\n",
    "# Create a key to help with the equivalent of a cross join\n",
    "tx['key'] = 0\n",
    "dts['key'] = 0\n",
    "\n",
    "# Create a dataset where all taxonomies have all traffic_dates from the first date where page_views > 0\n",
    "tx_dts = tx.merge(dts, on=\"key\", how=\"outer\").drop(columns='key')\n",
    "tx_dts = tx_dts.merge(tx_dt1, on=\"taxonomy\", how=\"inner\")\n",
    "tx_dts = tx_dts[tx_dts['traffic_date'] >= tx_dts['first_traffic_date']].drop(columns='first_traffic_date')\n",
    "\n",
    "# Merge tx_dts to tx_df2\n",
    "tx_df3 = tx_dts.merge(tx_df2, on=[\"taxonomy\", \"traffic_date\"], how=\"left\")\n",
    "print(tx_df3[\"page_views\"].isna().value_counts())\n",
    "print(tx_df3.shape[0])\n",
    "\n",
    "# Many \"missing\" observations: 21K out of 138K"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a3a2b63",
   "metadata": {},
   "source": [
    "Taxonomies may have missing values, zero values, and/or very few observations > 0\n",
    "Not all taxonomies are suitable for modeling\n",
    "Taxonomies with \"higher\" volumes and regular observations will generally produce more accurate forecasts\n",
    "Filter out unsuitable taxonomies\n",
    "Define arbitrary but reasonable filter criteria:\n",
    "Discard taxonomy from analysis if it meets any of the following rules: \n",
    "- If taxonomy has over 30% missing values\n",
    "- If taxonomy has max observation <= 5\n",
    "Note: values of zero are not \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features to tx_df3 to facilitate computation of filter rules\n",
    "missing_values_pct_threshold = 0.3\n",
    "max_observations_threshold = 5\n",
    "\n",
    "tx_df3['missing_values'] = 0\n",
    "tx_df3.loc[tx_df3['page_views'].isna() == True, 'missing_values'] = 1\n",
    "\n",
    "tx_df3['zero_values'] = 0\n",
    "tx_df3.loc[tx_df3['page_views'] == 0, 'zero_values'] = 1\n",
    "\n",
    "# Create DF to capture filter statistics at the taxonomy level\n",
    "tx_filter_summary = tx_df3['taxonomy'].value_counts().reset_index().rename(columns={\"index\": \"taxonomy\", \"taxonomy\": \"dt_cnt\"})\n",
    "\n",
    "# Add statistics to tx_filter_summary\n",
    "tx_filter_summary = tx_filter_summary.merge(tx_df3.groupby('taxonomy')['page_views'].max().reset_index().rename(columns={\"page_views\": \"max_pv\"}), on=\"taxonomy\", how=\"left\")\n",
    "tx_filter_summary = tx_filter_summary.merge(tx_df3.groupby('taxonomy')['missing_values'].sum().reset_index(), on=\"taxonomy\", how=\"left\")\n",
    "tx_filter_summary = tx_filter_summary.merge(tx_df3.groupby('taxonomy')['zero_values'].sum().reset_index(), on=\"taxonomy\", how=\"left\")\n",
    "tx_filter_summary['missing_values_pct'] = tx_filter_summary['missing_values']/tx_filter_summary['dt_cnt']\n",
    "tx_filter_summary['zero_values_pct'] = tx_filter_summary['zero_values']/tx_filter_summary['dt_cnt']\n",
    "\n",
    "# Identify taxonomies to keep for modeling \n",
    "tx_keep = tx_filter_summary[(tx_filter_summary['max_pv'] > max_observations_threshold) & (tx_filter_summary['missing_values_pct'] < missing_values_pct_threshold)]# & (tx_filter_summary['zero_values_pct'] < 0.1)]\n",
    "print(tx_keep.shape)\n",
    "\n",
    "# 86 out of 136 taxonomies retained for modeling\n",
    "# 50 out of 136 of taxonomies filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d54929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset with taxonomies to keep\n",
    "tx_df4 = tx_df3[tx_df3['taxonomy'].isin(tx_keep['taxonomy'])][['taxonomy', 'traffic_date', 'page_views']]\n",
    "print(tx_df4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91628e97",
   "metadata": {},
   "source": [
    "Handling the imputation of missing values\n",
    "- For sake of moving forward, use moving linear interpolation method; will jump over any analyses to find a \"best\" interpolation method, but it could be programmed and applied to each taxonomy\n",
    "- Other simple \"heuristic methods\" that could be used: mean imputation, median imputation, last observation carried forward, next observation carried backward, moving average interpolation\n",
    "\n",
    "Handle anomalies\n",
    "- Identify anomalies, remove them and replace them with moving average interpolation method\n",
    "\n",
    "Transform the data\n",
    "- Because of zero values, do not use log transformation, Box-Cox is a pretty good choice according to Rob Hyndman\n",
    "- Box-Cox does not work when any values = 0 so need to add 1 to all values before Box-Cox transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each taxonomy and fill in missing values using linear interpolation\n",
    "tx_df4['page_views_impute'] = tx_df4['page_views']\n",
    "\n",
    "for tx in tx_df4['taxonomy'].unique():\n",
    "    tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute'] = tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute'].interpolate(method='linear')\n",
    "\n",
    "print(tx_df4[tx_df4['page_views'].isna() == True]['taxonomy'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some taxonomy time series\n",
    "tx = 'workwear>cooling clothing & gear'\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title(f\"Page Views by Date {tx}\")\n",
    "plt.plot(tx_df4[tx_df4['taxonomy'] == tx]['traffic_date'], tx_df4[tx_df4['taxonomy'] == tx]['page_views_impute'], linewidth=1.5, color=\"black\")\n",
    "plt.plot(tx_df4[tx_df4['taxonomy'] == tx]['traffic_date'], tx_df4[tx_df4['taxonomy'] == tx]['page_views'], linewidth=0.5, color=\"lightblue\")\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb29121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View aggregated time series\n",
    "agg_df = tx_df4.groupby('traffic_date')['page_views_impute'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title('Page Views by Date')\n",
    "plt.plot(agg_df['page_views_impute'], linewidth=0.5, color=\"darkblue\")\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632047e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# What if page_views are aggregated by week?\n",
    "agg_df['traffic_week'] = agg_df['traffic_date'].dt.to_period('W').dt.start_time\n",
    "\n",
    "agg_df2 = agg_df.groupby('traffic_week')['page_views_impute'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title('Page Views by Week')\n",
    "plt.plot(agg_df2['traffic_week'], agg_df2['page_views_impute'], linewidth=0.5, color=\"darkblue\")\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()\n",
    "\n",
    "# Less noise\n",
    "# Stick with daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58148fa1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate through each taxonomy\n",
    "# Idenfity anomalies using Isolation Forest from sklearn\n",
    "# Replace them with linear interpolation imputations\n",
    "# Box-Cox transformation of resulting time series\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "# Set outliers fraction to 0.01, or 1% of the data, which is about 4 observations per year\n",
    "outliers_fraction = float(0.01)\n",
    "\n",
    "tx_df4['anomaly'] = 0\n",
    "tx_df4['page_views_impute_anomaly'] = tx_df4['page_views_impute']\n",
    "tx_df4['pv_ia_bc'] = 0\n",
    "\n",
    "boxcox_lambdas_df = pd.DataFrame(tx_df4['taxonomy'].unique(), columns={\"taxonomy\"})\n",
    "boxcox_lambdas_df['boxcox_lambda'] = 0\n",
    "\n",
    "for tx in tx_df4['taxonomy'].unique():\n",
    "    page_views = tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute']\n",
    "    page_views.index = tx_df4.loc[tx_df4['taxonomy'] == tx, 'traffic_date']\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    pv_scaled = scaler.fit_transform(page_views.values.reshape(-1, 1))\n",
    "    pv_scaled_df = pd.DataFrame(pv_scaled)\n",
    "\n",
    "    # train isolation forest\n",
    "    model =  IsolationForest(contamination=outliers_fraction)\n",
    "    model.fit(pv_scaled_df)\n",
    "\n",
    "    page_views = pd.DataFrame(page_views)\n",
    "    page_views['anomaly'] = model.predict(pv_scaled_df)\n",
    "\n",
    "    page_views = page_views.reset_index()\n",
    "    page_views.index = tx_df4[tx_df4['taxonomy'] == tx].index\n",
    "\n",
    "    # Interpolate new value for anomalies\n",
    "    # Identify anomalies in tx_df4\n",
    "    tx_df4.loc[tx_df4['taxonomy'] == tx, 'anomaly'] = page_views['anomaly']\n",
    "    # Set page_views_impute to na where anomaly = -1\n",
    "    tx_df4.loc[(tx_df4['taxonomy'] == tx) & (tx_df4['anomaly'] == -1), 'page_views_impute_anomaly'] = np.nan\n",
    "    # Interpolate new nans\n",
    "    tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'] = tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'].interpolate(method='linear')\n",
    "    # Box-Cox transformation\n",
    "    tx_df4.loc[tx_df4['taxonomy'] == tx, 'pv_ia_bc'], boxcox_lambdas_df.loc[boxcox_lambdas_df['taxonomy'] == tx, 'boxcox_lambda'] = boxcox(tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'] + 1)\n",
    "    \n",
    "    # visualization\n",
    "    plt.figure(figsize=(12,5))\n",
    "    ax = plt.axes()\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "    plt.title(f\"Page View Anomalies {tx}\")\n",
    "    plt.plot(tx_df4.loc[tx_df4['taxonomy'] == tx, 'traffic_date'], tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute'], linewidth=0.5, color=\"lightblue\", label='Normal')\n",
    "    plt.plot(tx_df4.loc[tx_df4['taxonomy'] == tx, 'traffic_date'], tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'], linewidth=0.5, color=\"darkblue\", label='Normal')\n",
    "    plt.scatter(tx_df4.loc[(tx_df4['taxonomy'] == tx) & (tx_df4['anomaly'] == -1), 'traffic_date'], tx_df4.loc[(tx_df4['taxonomy'] == tx) & (tx_df4['anomaly'] == -1), 'page_views_impute'], color=\"red\", label='Anomaly')\n",
    "    plt.xticks(rotation = 45, fontsize=8)\n",
    "    plt.grid(color=\"white\")\n",
    "    plt.show()\n",
    "\n",
    "#     # visualization\n",
    "#     plt.figure(figsize=(12,5))\n",
    "#     ax = plt.axes()\n",
    "#     ax.set_facecolor(\"whitesmoke\")\n",
    "#     plt.title(f\"Page View Box-Cox {tx}\")\n",
    "#     plt.plot(tx_df4.loc[tx_df4['taxonomy'] == tx, 'traffic_date'], tx_df4.loc[tx_df4['taxonomy'] == tx, 'pv_ia_bc'], linewidth=0.5, color=\"blue\")\n",
    "#     plt.xticks(rotation = 45, fontsize=8)\n",
    "#     plt.grid(color=\"white\")\n",
    "#     plt.show()\n",
    "\n",
    "# Looks reasonable\n",
    "\n",
    "# Some page_views_impute_anomaly are NA and page_views_impute are not NA\n",
    "# In those cases, set page_views_impute_anomaly = page_views_impute\n",
    "tx_df4.loc[(tx_df4['page_views_impute_anomaly'].isna() == True) & (tx_df4['page_views_impute'].isna() == False), 'page_views_impute_anomaly'] = tx_df4.loc[(tx_df4['page_views_impute_anomaly'].isna() == True) & (tx_df4['page_views_impute'].isna() == False), 'page_views_impute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validate inverse boxcox function\n",
    "# tx_df4['pv_ia_bc_inv'] = 0\n",
    "\n",
    "# for tx in tx_df4['taxonomy'].unique():\n",
    "#     lmbda = float(boxcox_lambdas_df.loc[boxcox_lambdas_df['taxonomy'] == tx, 'boxcox_lambda'])\n",
    "#     tx_df4.loc[tx_df4['taxonomy'] == tx, 'pv_ia_bc_inv'] = inv_boxcox(tx_df4.loc[tx_df4['taxonomy'] == tx, 'pv_ia_bc'], lmbda) - 1\n",
    "\n",
    "# # It works"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67b0a64a",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "\n",
    "From date:\n",
    "- Year\n",
    "- Month\n",
    "- 1/2 Month (1-15 of month is one half, 16-EOM is second half)\n",
    "- Day of week indicators\n",
    "- Weekend indicator\n",
    "- Week of year? Problematic when most years have 52 weeks and some have 53\n",
    "- Days before Christmas\n",
    "- Days before 4th of July\n",
    "\n",
    "From page views:\n",
    "- Lag 1 day, 7 days, 30 days\n",
    "- Moving average of past 7 days (7-day MA)\n",
    "- Moving average of past 30 days (30-day MA)\n",
    "- 7-day MA Lag 1 day, 7 days, 30 days\n",
    "- 30-day MA Lag 1 day, 7 days, 30 days"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9692ed93",
   "metadata": {},
   "source": [
    "Many different patterns in time series\n",
    "Train a handful of models on each taxonomy, use the best as the final forecast\n",
    "\n",
    "Models/Model Packages to test:\n",
    "- LightGBM, this model performed well in the M5 competition\n",
    "- AutoARIMA (is actually SARIMA)\n",
    "- Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a378e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add engineered date features\n",
    "tx_df_fe = tx_df4\n",
    "tx_df_fe['year'] = tx_df_fe['traffic_date'].dt.year\n",
    "tx_df_fe['quarter'] = tx_df_fe['traffic_date'].dt.quarter\n",
    "tx_df_fe['month'] = tx_df_fe['traffic_date'].dt.month\n",
    "tx_df_fe['half_month'] = tx_df_fe['month'] * 2\n",
    "tx_df_fe.loc[tx_df_fe['traffic_date'].dt.day <= 15, 'half_month'] = tx_df_fe.loc[tx_df_fe['traffic_date'].dt.day <= 15, 'half_month'] - 1\n",
    "tx_df_fe['dayofweek'] = tx_df_fe['traffic_date'].dt.dayofweek\n",
    "tx_df_fe['weekend'] = 0\n",
    "tx_df_fe.loc[tx_df_fe['dayofweek'] >= 5, 'weekend'] = 1\n",
    "\n",
    "next_xmas_year = tx_df_fe['traffic_date'].dt.year\n",
    "next_xmas_year.loc[tx_df_fe['traffic_date'] > pd.to_datetime(dict(year=tx_df_fe['traffic_date'].dt.year, month=12, day=25))] = next_xmas_year.loc[tx_df_fe['traffic_date'] > pd.to_datetime(dict(year=tx_df_fe['traffic_date'].dt.year, month=12, day=25))] + 1\n",
    "\n",
    "tx_df_fe['days_until_xmas'] = (pd.to_datetime(dict(year=next_xmas_year, month=12, day=25)) - tx_df_fe['traffic_date']).dt.days\n",
    "\n",
    "next_jul4_year = tx_df_fe['traffic_date'].dt.year\n",
    "next_jul4_year.loc[tx_df_fe['traffic_date'] > pd.to_datetime(dict(year=tx_df_fe['traffic_date'].dt.year, month=7, day=4))] = next_jul4_year.loc[tx_df_fe['traffic_date'] > pd.to_datetime(dict(year=tx_df_fe['traffic_date'].dt.year, month=7, day=4))] + 1\n",
    "\n",
    "tx_df_fe['days_until_jul4'] = (pd.to_datetime(dict(year=next_jul4_year, month=7, day=4)) - tx_df_fe['traffic_date']).dt.days\n",
    "\n",
    "print(tx_df_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728474f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add engineered page view features\n",
    "# 7-day MA\n",
    "# 30-day MA\n",
    "# Lag 1, 7 and 30\n",
    "\n",
    "tx_df_fe['pv_ia_lag1'] = 0\n",
    "tx_df_fe['pv_ia_lag7'] = 0\n",
    "tx_df_fe['pv_ia_lag30'] = 0\n",
    "\n",
    "tx_df_fe['pv_ia_ma7'] = 0\n",
    "tx_df_fe['pv_ia_ma30'] = 0\n",
    "\n",
    "tx_df_fe['pv_ia_ma7_lag1'] = 0\n",
    "tx_df_fe['pv_ia_ma7_lag7'] = 0\n",
    "tx_df_fe['pv_ia_ma7_lag30'] = 0\n",
    "tx_df_fe['pv_ia_ma30_lag1'] = 0\n",
    "tx_df_fe['pv_ia_ma30_lag7'] = 0\n",
    "tx_df_fe['pv_ia_ma30_lag30'] = 0\n",
    "\n",
    "\n",
    "for tx in tx_df_fe['taxonomy'].unique():\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_lag1'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'].shift(1)\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_lag7'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'].shift(7)\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_lag30'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'].shift(30)\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma7'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'].rolling(window=7, min_periods=1).mean()\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma30'] = tx_df4.loc[tx_df4['taxonomy'] == tx, 'page_views_impute_anomaly'].rolling(window=30, min_periods=1).mean()    \n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma7_lag1'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma7'].shift(1)\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma7_lag7'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma7'].shift(7)\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma7_lag30'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma7'].shift(30)\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma30_lag1'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma30'].shift(1)\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma30_lag7'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma30'].shift(7)\n",
    "    tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma30_lag30'] = tx_df_fe.loc[tx_df4['taxonomy'] == tx, 'pv_ia_ma30'].shift(30)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    ax = plt.axes()\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "    plt.title(f\"Page View Features {tx}\")\n",
    "    plt.plot(tx_df_fe.loc[tx_df_fe['taxonomy'] == tx, 'traffic_date'], tx_df_fe.loc[tx_df_fe['taxonomy'] == tx, 'page_views_impute_anomaly'], color=\"blue\", linewidth=0.5)\n",
    "    plt.plot(tx_df_fe.loc[tx_df_fe['taxonomy'] == tx, 'traffic_date'], tx_df_fe.loc[tx_df_fe['taxonomy'] == tx, 'pv_ia_ma7'], color=\"black\")\n",
    "    plt.plot(tx_df_fe.loc[tx_df_fe['taxonomy'] == tx, 'traffic_date'], tx_df_fe.loc[tx_df_fe['taxonomy'] == tx, 'pv_ia_ma30'], color=\"red\")\n",
    "    plt.xticks(rotation = 45, fontsize=8)\n",
    "    plt.grid(color=\"white\")\n",
    "    plt.show()\n",
    "\n",
    "print(tx_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b03ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test lightgbm forecasting\n",
    "import lightgbm as lgb\n",
    "from mlforecast import MLForecast\n",
    "\n",
    "model = lgb.LGBMRegressor(random_state=50)\n",
    "\n",
    "\n",
    "gbm_features = ['traffic_date',\n",
    "                'page_views_impute_anomaly',\n",
    "                'pv_ia_ma7',\n",
    "                'pv_ia_ma30',\n",
    "                'year',\n",
    "                'quarter',\n",
    "                'month',\n",
    "                'dayofweek',\n",
    "                'weekend',\n",
    "                'days_until_xmas',\n",
    "                'days_until_jul4',\n",
    "                'pv_ia_lag1',\n",
    "                'pv_ia_lag7',\n",
    "                'pv_ia_lag30',\n",
    "                'pv_ia_ma7_lag1',\n",
    "                'pv_ia_ma7_lag7',\n",
    "                'pv_ia_ma7_lag30',\n",
    "                'pv_ia_ma30_lag1',\n",
    "                'pv_ia_ma30_lag7',\n",
    "                'pv_ia_ma30_lag30'\n",
    "               ]\n",
    "\n",
    "gbm_X = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[4:]]\n",
    "gbm_X.index = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[0]]\n",
    "gbm_y = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[1]]\n",
    "gbm_y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[0]]\n",
    "\n",
    "train_X = gbm_X.iloc[:-182, :]\n",
    "train_y = gbm_y.iloc[:-182]\n",
    "test_X = gbm_X.iloc[-182:, :]\n",
    "test_y = gbm_y.iloc[-182:]\n",
    "\n",
    "# Iterate forecast one day at a time\n",
    "# Get forecast for next day, then calculate lag features using the forecast\n",
    "\n",
    "lgbm_fit = lgb.LGBMRegressor(random_state=432, num_leaves=100, min_child_samples=4)\n",
    "lgbm_fit.fit(train_X, train_y)\n",
    "\n",
    "lgb.plot_importance(lgbm_fit, importance_type = 'split')\n",
    "plt.show()\n",
    "\n",
    "train_y_pred = pd.Series(lgbm_fit.predict(train_X), index=train_y.index)\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title(f\"Page Views Forecast {tx}\")\n",
    "plt.plot(train_y, color = \"black\", linewidth=2)\n",
    "plt.plot(train_y_pred, color = \"yellow\", linewidth=0.5)\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training MdAPE: {(abs(train_y_pred/train_y - 1)).median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features].set_index('traffic_date')\n",
    "history['train_test'] = 'train'\n",
    "history.loc[history.index >= test_X.index[0], 'train_test'] = 'test'\n",
    "history['pv_ia_pred'] = 0\n",
    "history.loc[history['train_test'] == 'train', 'pv_ia_pred'] = train_y_pred\n",
    "history['pv_ia_history'] = history['page_views_impute_anomaly']\n",
    "\n",
    "for i in history[history['train_test'] == 'test'].index:\n",
    "    # Populate lagged input features\n",
    "    history.loc[history.index == i, \"pv_ia_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_history\"][0]\n",
    "    history.loc[history.index == i, \"pv_ia_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_history\"][0]\n",
    "    history.loc[history.index == i, \"pv_ia_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_history\"][0]\n",
    "    history.loc[history.index == i, \"pv_ia_ma7_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_ma7\"][0]\n",
    "    history.loc[history.index == i, \"pv_ia_ma7_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_ma7\"][0]\n",
    "    history.loc[history.index == i, \"pv_ia_ma7_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_ma7\"][0]\n",
    "    history.loc[history.index == i, \"pv_ia_ma30_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_ma30\"][0]\n",
    "    history.loc[history.index == i, \"pv_ia_ma30_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_ma30\"][0]\n",
    "    history.loc[history.index == i, \"pv_ia_ma30_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_ma30\"][0]\n",
    "    \n",
    "    # Predict next page_view count, populate in pv_ia_history\n",
    "    y_hat = lgbm_fit.predict(pd.DataFrame(history.loc[history.index == i, gbm_features[4:]]))[0]\n",
    "    history.loc[history.index == i, \"pv_ia_history\"] = y_hat\n",
    "    history.loc[history.index == i, \"pv_ia_pred\"] = y_hat\n",
    "    \n",
    "    # Populate latest ma7 and ma30 with pv_ia_history\n",
    "    history.loc[history.index == i, \"pv_ia_ma7\"] = history.loc[(history.index <= i) & (history.index >= i - pd.Timedelta(days=6)), \"pv_ia_history\"].mean()\n",
    "    history.loc[history.index == i, \"pv_ia_ma30\"] = history.loc[(history.index <= i) & (history.index >= i - pd.Timedelta(days=29)), \"pv_ia_history\"].mean()\n",
    "    \n",
    "# Plot actual vs. pred / forecast\n",
    "plt.figure(figsize=(16,5))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title(f\"Page Views Forecast {tx}\")\n",
    "plt.plot(history['page_views_impute_anomaly'], color = \"black\", linewidth=2)\n",
    "plt.plot(history['pv_ia_pred'], color = \"yellow\", linewidth=0.5)\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to weekly level and get MAPE\n",
    "history = history.reset_index()\n",
    "history['traffic_week'] = history['traffic_date'].dt.to_period('W').dt.start_time\n",
    "\n",
    "history_wk = history.groupby(['traffic_week', 'train_test'])[['page_views_impute_anomaly', 'pv_ia_pred']].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title(f\"Page Views by Week for {tx}\")\n",
    "plt.plot(history_wk['traffic_week'], history_wk['page_views_impute_anomaly'], linewidth=2, color=\"darkblue\")\n",
    "plt.plot(history_wk['traffic_week'], history_wk['pv_ia_pred'], linewidth=0.5, color=\"orange\")\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8154887",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get weekly MAPE\n",
    "mape_lgbm = abs(history_wk[history_wk['train_test'] == 'test']['pv_ia_pred']/history_wk[history_wk['train_test'] == 'test']['page_views_impute_anomaly'] - 1).mean()\n",
    "print(mape_lgbm)\n",
    "\n",
    "# Get daily MAPE\n",
    "mape_lgbm_daily = abs(history[history['train_test'] == 'test']['pv_ia_pred']/history[history['train_test'] == 'test']['page_views_impute_anomaly'] - 1).mean()\n",
    "print(mape_lgbm_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342ee9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test AutoARIMA\n",
    "# from pmdarima.arima import AutoARIMA\n",
    "\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "\n",
    "y = tx_df_fe[tx_df_fe['taxonomy'] == tx]['pv_ia_bc']\n",
    "y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx]['traffic_date']\n",
    "\n",
    "train_X = gbm_X.iloc[:-182, :]\n",
    "train_y = gbm_y.iloc[:-182]\n",
    "test_X = gbm_X.iloc[-182:, :]\n",
    "test_y = gbm_y.iloc[-182:]\n",
    "\n",
    "y_train = y.iloc[:-182]\n",
    "y_test = y.iloc[-182:]\n",
    "\n",
    "forecast_dates = pd.Series(pd.date_range(tx_df_fe['traffic_date'].max(), periods=183)[1:])\n",
    "print(forecast_dates)\n",
    "\n",
    "arima_fit = pm.auto_arima(y_train,\n",
    "                          max_p = 30,\n",
    "                          max_P = 366,\n",
    "                          error_action='ignore',\n",
    "                          trace=True,\n",
    "                          suppress_warnings=True,\n",
    "                          maxiter=5,\n",
    "                          random_state=120)\n",
    "\n",
    "y_train_pred = pd.Series(arima_fit.predict_in_sample())\n",
    "y_train_pred.index = y_train.index\n",
    "y_test_pred = pd.Series(arima_fit.predict(182))\n",
    "y_test_pred.index = y_test.index\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "plt.title(f\"Page Views Forecast {tx}\")\n",
    "plt.plot(y_train, color = \"black\", linewidth=2)\n",
    "plt.plot(y_train_pred, color = \"blue\", linewidth=0.5)\n",
    "plt.plot(y_test_pred, color = \"green\", linewidth=0.5)\n",
    "plt.xticks(rotation = 45, fontsize=8)\n",
    "plt.grid(color=\"white\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Aggregate to weekly level and get MAPE\n",
    "arima_df = pd.concat([y_test, y_test_pred], axis = 1).reset_index()\n",
    "arima_df.columns = ['traffic_date', 'pv_ia_bc', 'pv_ia_bc_forecast']\n",
    "\n",
    "lmbda = float(boxcox_lambdas_df.loc[boxcox_lambdas_df['taxonomy'] == tx, 'boxcox_lambda'])\n",
    "print(lmbda)\n",
    "arima_df['pv_ia'] = inv_boxcox(arima_df['pv_ia_bc'], lmbda) - 1\n",
    "arima_df['pv_ia_forecast'] = inv_boxcox(arima_df['pv_ia_bc_forecast'], lmbda) - 1\n",
    "\n",
    "print(arima_df)\n",
    "\n",
    "arima_df['traffic_week'] = arima_df['traffic_date'].dt.to_period('W').dt.start_time\n",
    "\n",
    "arima_df_wk = arima_df.groupby(['traffic_week'])[['pv_ia', 'pv_ia_forecast']].sum().reset_index()\n",
    "\n",
    "\n",
    "# Get weekly MAPE\n",
    "mape_arima = abs(arima_df_wk['pv_ia_forecast']/arima_df_wk['pv_ia'] - 1).mean()\n",
    "print(mape_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FB Prophet\n",
    "from prophet import Prophet\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "y = tx_df_fe[tx_df_fe['taxonomy'] == tx]['page_views_impute_anomaly']\n",
    "y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx]['traffic_date']\n",
    "\n",
    "y = y.reset_index()\n",
    "y.columns = [\"ds\", \"y\"]\n",
    "\n",
    "y_train = y.iloc[:-182, :]\n",
    "\n",
    "\n",
    "prophet_fit = Prophet(daily_seasonality=True)\n",
    "prophet_fit.fit(y_train)\n",
    "\n",
    "future = prophet_fit.make_future_dataframe(periods=182)\n",
    "\n",
    "forecast = prophet_fit.predict(future)\n",
    "\n",
    "print(forecast[['ds', 'yhat']])\n",
    "\n",
    "# fig1 = prophet_fit.plot(forecast)\n",
    "# fig2 = prophet_fit.plot_components(forecast)\n",
    "\n",
    "# plot_plotly(prophet_fit, forecast)\n",
    "# plot_components_plotly(prophet_fit, forecast)\n",
    "y.columns = ['traffic_date', 'page_views_impute_anomaly']\n",
    "\n",
    "y['yhat'] = forecast['yhat']\n",
    "\n",
    "y['train_test'] = 'train'\n",
    "\n",
    "y.loc[y.index >= y.index.max() - 181, 'train_test'] = 'test'\n",
    "\n",
    "y['traffic_week'] = y['traffic_date'].dt.to_period('W').dt.start_time\n",
    "\n",
    "y_wk = y.groupby(['traffic_week', 'train_test'])[['page_views_impute_anomaly', 'yhat']].sum().reset_index()\n",
    "\n",
    "\n",
    "# Get weekly MAPE\n",
    "mape_prophet = abs(y_wk[y_wk['train_test'] == 'test']['yhat']/y_wk[y_wk['train_test'] == 'test']['page_views_impute_anomaly'] - 1).mean()\n",
    "print(mape_prophet)\n",
    "\n",
    "# Get daily MAPE\n",
    "mape_prophet_daily = abs(y[y['train_test'] == 'test']['yhat']/y[y['train_test'] == 'test']['page_views_impute_anomaly'] - 1).mean()\n",
    "print(mape_prophet_daily)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea4c76da",
   "metadata": {},
   "source": [
    "Iterate through each taxonomy, train LGBM, Auto ARIMA, and Prophet models\n",
    "Train on all the data given except that last 182 days (26 weeks, which is about 6 months)\n",
    "Test set is the last 182 days in the given data\n",
    "Calculate MAPE for each model\n",
    "MAPE is based on test set forecast vs. actuals aggregated at weekly level\n",
    "Use best model as final model, train on all data given, forecast out 182 days\n",
    "Aggregate forecasts per week\n",
    "Devise 80% confidence intervals from daily to weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6602d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe for final forecasts\n",
    "# To include:\n",
    "# - taxonomy\n",
    "# - week_end_date\n",
    "# - train or test\n",
    "# - page_views (original)\n",
    "# - forecast page_views\n",
    "# - confidence intervals of forecasts\n",
    "# - forecast model\n",
    "\n",
    "# Last forecast date is 2024-07-21\n",
    "\n",
    "actual_days = len(tx_df_fe['traffic_date'].unique())\n",
    "last_actual_day = tx_df_fe['traffic_date'].max()\n",
    "traffic_dates = pd.DataFrame(pd.Series(pd.date_range(tx_df_fe['traffic_date'].min(), periods=actual_days + 182))).rename(columns={0: \"traffic_date\"})\n",
    "traffic_dates['week_end_date'] = pd.to_datetime(traffic_dates['traffic_date'].dt.to_period('W').dt.end_time.dt.date)\n",
    "traffic_dates['key'] = 0\n",
    "traffic_dates['actual_forecast'] = 'actual'\n",
    "traffic_dates.loc[traffic_dates['traffic_date'] > last_actual_day, 'actual_forecast'] = 'forecast'\n",
    "\n",
    "txs = pd.DataFrame(pd.Series(tx_df_fe['taxonomy'].unique())).rename(columns={0: 'taxonomy'})\n",
    "txs['key'] = 0\n",
    "\n",
    "final_daily_df = txs.merge(traffic_dates, on='key', how='outer').drop(columns='key')\n",
    "final_daily_df = final_daily_df.merge(tx_df_fe[['taxonomy', 'traffic_date', 'page_views', 'page_views_impute_anomaly']], on=['taxonomy', 'traffic_date'], how='left')\n",
    "\n",
    "final_daily_df['page_views_forecast'] = np.nan\n",
    "final_daily_df['page_views_forecast_lower'] = np.nan\n",
    "final_daily_df['page_views_forecast_upper'] = np.nan\n",
    "final_daily_df['forecast_model'] = np.nan\n",
    "\n",
    "final_daily_df['keep'] = 1\n",
    "final_daily_df.loc[(final_daily_df['actual_forecast'] == 'actual') & (final_daily_df['page_views_impute_anomaly'].isna() == True), 'keep'] = 0\n",
    "final_daily_df = final_daily_df[final_daily_df['keep'] == 1].drop(columns='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_features = ['traffic_date',\n",
    "                'page_views_impute_anomaly',\n",
    "                'pv_ia_ma7',\n",
    "                'pv_ia_ma30',\n",
    "                'year',\n",
    "                'quarter',\n",
    "                'month',\n",
    "                'dayofweek',\n",
    "                'weekend',\n",
    "                'days_until_xmas',\n",
    "                'days_until_jul4',\n",
    "                'pv_ia_lag1',\n",
    "                'pv_ia_lag7',\n",
    "                'pv_ia_lag30',\n",
    "                'pv_ia_ma7_lag1',\n",
    "                'pv_ia_ma7_lag7',\n",
    "                'pv_ia_ma7_lag30',\n",
    "                'pv_ia_ma30_lag1',\n",
    "                'pv_ia_ma30_lag7',\n",
    "                'pv_ia_ma30_lag30'\n",
    "               ]\n",
    "\n",
    "# Function to get weekly test MAPE of LGBM\n",
    "def lgbm_eval(tx_df_fe: pd.DataFrame, gbm_features: list, tx: str):\n",
    "\n",
    "    gbm_X = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[4:]]\n",
    "\n",
    "    if gbm_X.shape[0] > 365:\n",
    "        gbm_X.index = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[0]]\n",
    "        gbm_y = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[1]]\n",
    "        gbm_y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[0]]\n",
    "\n",
    "        train_X = gbm_X.iloc[:-182, :]\n",
    "        train_y = gbm_y.iloc[:-182]\n",
    "        test_X = gbm_X.iloc[-182:, :]\n",
    "        test_y = gbm_y.iloc[-182:]\n",
    "\n",
    "        # Iterate forecast one day at a time\n",
    "        # Get forecast for next day, then calculate lag features using the forecast\n",
    "\n",
    "        lgbm_fit = lgb.LGBMRegressor(random_state=432, min_child_samples=4)\n",
    "        lgbm_fit.fit(train_X, train_y)\n",
    "\n",
    "        train_y_pred = pd.Series(lgbm_fit.predict(train_X), index=train_y.index)\n",
    "        \n",
    "        history = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features].set_index('traffic_date')\n",
    "        history['train_test'] = 'train'\n",
    "        history.loc[history.index >= test_X.index.min(), 'train_test'] = 'test'\n",
    "        history['pv_ia_pred'] = 0\n",
    "        history.loc[history['train_test'] == 'train', 'pv_ia_pred'] = train_y_pred\n",
    "        history['pv_ia_history'] = history['page_views_impute_anomaly']\n",
    "\n",
    "        for i in history[history['train_test'] == 'test'].index:\n",
    "            # Populate lagged input features\n",
    "            history.loc[history.index == i, \"pv_ia_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_history\"][0]\n",
    "            history.loc[history.index == i, \"pv_ia_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_history\"][0]\n",
    "            history.loc[history.index == i, \"pv_ia_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_history\"][0]\n",
    "            history.loc[history.index == i, \"pv_ia_ma7_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_ma7\"][0]\n",
    "            history.loc[history.index == i, \"pv_ia_ma7_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_ma7\"][0]\n",
    "            history.loc[history.index == i, \"pv_ia_ma7_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_ma7\"][0]\n",
    "            history.loc[history.index == i, \"pv_ia_ma30_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_ma30\"][0]\n",
    "            history.loc[history.index == i, \"pv_ia_ma30_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_ma30\"][0]\n",
    "            history.loc[history.index == i, \"pv_ia_ma30_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_ma30\"][0]\n",
    "\n",
    "            # Predict next page_view count, populate in pv_ia_history\n",
    "            y_hat = lgbm_fit.predict(pd.DataFrame(history.loc[history.index == i, gbm_features[4:]]))[0]\n",
    "            history.loc[history.index == i, \"pv_ia_history\"] = y_hat\n",
    "            history.loc[history.index == i, \"pv_ia_pred\"] = y_hat\n",
    "\n",
    "            # Populate latest ma7 and ma30 with pv_ia_history\n",
    "            history.loc[history.index == i, \"pv_ia_ma7\"] = history.loc[(history.index <= i) & (history.index >= i - pd.Timedelta(days=6)), \"pv_ia_history\"].mean()\n",
    "            history.loc[history.index == i, \"pv_ia_ma30\"] = history.loc[(history.index <= i) & (history.index >= i - pd.Timedelta(days=29)), \"pv_ia_history\"].mean()\n",
    "\n",
    "        history = history.reset_index()\n",
    "        history['traffic_week'] = history['traffic_date'].dt.to_period('W').dt.start_time\n",
    "\n",
    "        history_wk = history.groupby(['traffic_week', 'train_test'])[['page_views_impute_anomaly', 'pv_ia_pred']].sum().reset_index()\n",
    "\n",
    "        # Get weekly MAPE\n",
    "        mape_lgbm = round(abs(history_wk[history_wk['train_test'] == 'test']['pv_ia_pred']/history_wk[history_wk['train_test'] == 'test']['page_views_impute_anomaly'] - 1).mean(), 5)\n",
    "    else:\n",
    "        mape_lgbm = np.inf\n",
    "    return mape_lgbm\n",
    "\n",
    "# Function to get final LGBM forecasts\n",
    "def lgbm_final(tx_df_fe: pd.DataFrame, gbm_features: list, tx: str, final_daily_df: pd.DataFrame):\n",
    "\n",
    "    gbm_features_trim = gbm_features.copy()\n",
    "    gbm_features_trim.remove('page_views_impute_anomaly')\n",
    "    \n",
    "    gbm_X = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[4:]]\n",
    "    gbm_X.index = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[0]]\n",
    "    gbm_y = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[1]]\n",
    "    gbm_y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features[0]]\n",
    "\n",
    "    # Iterate forecast one day at a time\n",
    "    # Get forecast for next day, then calculate lag features using the forecast\n",
    "\n",
    "    lgbm_fit = lgb.LGBMRegressor(random_state=432, min_child_samples=4)\n",
    "    lgbm_fit.fit(gbm_X, gbm_y)\n",
    "\n",
    "    lgbm_fit_lb = lgb.LGBMRegressor(random_state=396, min_child_samples_=4, objective = 'quantile', alpha = 0.1)\n",
    "    lgbm_fit_lb.fit(gbm_X, gbm_y)\n",
    "\n",
    "    lgbm_fit_ub = lgb.LGBMRegressor(random_state=396, min_child_samples_=4, objective = 'quantile', alpha = 0.9)\n",
    "    lgbm_fit_ub.fit(gbm_X, gbm_y)\n",
    "\n",
    "    gbm_y_pred = pd.Series(lgbm_fit.predict(gbm_X), index=gbm_y.index)\n",
    "    gbm_y_pred_lb = pd.Series(lgbm_fit_lb.predict(gbm_X), index=gbm_y.index)\n",
    "    gbm_y_pred_ub = pd.Series(lgbm_fit_ub.predict(gbm_X), index=gbm_y.index)\n",
    "\n",
    "    hist_index = final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'traffic_date']\n",
    "    hist_df1 = final_daily_df[final_daily_df['taxonomy'] == tx].reset_index(drop=True)\n",
    "    hist_df2 = tx_df_fe[tx_df_fe['taxonomy'] == tx][gbm_features_trim].reset_index(drop=True)\n",
    "    history = hist_df1.merge(hist_df2, on=['traffic_date'], how='left').set_index(hist_index)\n",
    "\n",
    "    history['pv_ia_history'] = history['page_views_impute_anomaly']\n",
    "    history.loc[history['actual_forecast'] == 'actual', 'page_views_forecast'] = gbm_y_pred\n",
    "    history.loc[history['actual_forecast'] == 'actual', 'page_views_forecast_lower'] = gbm_y_pred_lb\n",
    "    history.loc[history['actual_forecast'] == 'actual', 'page_views_forecast_upper'] = gbm_y_pred_ub\n",
    "\n",
    "\n",
    "    for i in history[history['actual_forecast'] == 'forecast']['traffic_date']:\n",
    "\n",
    "        # Populate lagged input features\n",
    "        history.loc[history.index == i, \"pv_ia_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_history\"][0]\n",
    "        history.loc[history.index == i, \"pv_ia_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_history\"][0]\n",
    "        history.loc[history.index == i, \"pv_ia_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_history\"][0]\n",
    "        history.loc[history.index == i, \"pv_ia_ma7_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_ma7\"][0]\n",
    "        history.loc[history.index == i, \"pv_ia_ma7_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_ma7\"][0]\n",
    "        history.loc[history.index == i, \"pv_ia_ma7_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_ma7\"][0]\n",
    "        history.loc[history.index == i, \"pv_ia_ma30_lag1\"] = history.loc[history.index == i - pd.Timedelta(days=1), \"pv_ia_ma30\"][0]\n",
    "        history.loc[history.index == i, \"pv_ia_ma30_lag7\"] = history.loc[history.index == i - pd.Timedelta(days=7), \"pv_ia_ma30\"][0]\n",
    "        history.loc[history.index == i, \"pv_ia_ma30_lag30\"] = history.loc[history.index == i - pd.Timedelta(days=30), \"pv_ia_ma30\"][0]\n",
    "\n",
    "        # Predict next page_view count, populate in pv_ia_history\n",
    "        y_hat = lgbm_fit.predict(pd.DataFrame(history.loc[history.index == i, gbm_features[4:]]))[0]\n",
    "        history.loc[history.index == i, \"pv_ia_history\"] = y_hat\n",
    "        history.loc[history.index == i, \"page_views_forecast\"] = y_hat\n",
    "        y_hat_lower = lgbm_fit_lb.predict(pd.DataFrame(history.loc[history.index == i, gbm_features[4:]]))[0]\n",
    "        history.loc[history.index == i, \"page_views_forecast_lower\"] = y_hat_lower\n",
    "        y_hat_upper = lgbm_fit_ub.predict(pd.DataFrame(history.loc[history.index == i, gbm_features[4:]]))[0]\n",
    "        history.loc[history.index == i, \"page_views_forecast_upper\"] = y_hat_upper\n",
    "\n",
    "        # Populate latest ma7 and ma30 with pv_ia_history\n",
    "        history.loc[history.index == i, \"pv_ia_ma7\"] = history.loc[(history.index <= i) & (history.index >= i - pd.Timedelta(days=6)), \"pv_ia_history\"].mean()\n",
    "        history.loc[history.index == i, \"pv_ia_ma30\"] = history.loc[(history.index <= i) & (history.index >= i - pd.Timedelta(days=29)), \"pv_ia_history\"].mean()\n",
    "\n",
    "    history.index = final_daily_df[final_daily_df['taxonomy'] == tx].index\n",
    "\n",
    "    # Add taxonomy data to final_daily_df\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'page_views_forecast'] = history['page_views_forecast']\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'page_views_forecast_lower'] = history['page_views_forecast_lower']\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'page_views_forecast_upper'] = history['page_views_forecast_upper']\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'forecast_model'] = 'LGBM'\n",
    "\n",
    "    return final_daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Auto ARIMA MAPE\n",
    "def arima_eval(tx_df_fe: pd.DataFrame, boxcox_lambdas_df: pd.DataFrame, tx: str):\n",
    "\n",
    "    y = tx_df_fe[tx_df_fe['taxonomy'] == tx]['pv_ia_bc']\n",
    "    if y.shape[0] >= 365:\n",
    "        y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx]['traffic_date']\n",
    "\n",
    "        y_train = y.iloc[:-182]\n",
    "        y_test = y.iloc[-182:]\n",
    "\n",
    "        arima_fit = pm.auto_arima(y_train,\n",
    "                                  max_p = 30,\n",
    "                                  max_P = 366,\n",
    "                                  error_action='ignore',\n",
    "                                  trace=True,\n",
    "                                  suppress_warnings=True,\n",
    "                                  maxiter=5,\n",
    "                                  random_state=120)\n",
    "\n",
    "        y_train_pred = pd.Series(arima_fit.predict_in_sample())\n",
    "        y_train_pred.index = y_train.index\n",
    "        y_test_pred = pd.Series(arima_fit.predict(182))\n",
    "        y_test_pred.index = y_test.index\n",
    "\n",
    "        # Aggregate to weekly level and get MAPE\n",
    "        lmbda = float(boxcox_lambdas_df.loc[boxcox_lambdas_df['taxonomy'] == tx, 'boxcox_lambda'])\n",
    "\n",
    "        arima_df = pd.concat([y_test, y_test_pred], axis = 1).reset_index()\n",
    "        arima_df.columns = ['traffic_date', 'pv_ia_bc', 'pv_ia_bc_forecast']\n",
    "        arima_df['pv_ia'] = inv_boxcox(arima_df['pv_ia_bc'], lmbda) - 1\n",
    "        arima_df['pv_ia_forecast'] = inv_boxcox(arima_df['pv_ia_bc_forecast'], lmbda) - 1\n",
    "        arima_df['traffic_week'] = arima_df['traffic_date'].dt.to_period('W').dt.start_time\n",
    "        arima_df_wk = arima_df.groupby(['traffic_week'])[['pv_ia', 'pv_ia_forecast']].sum().reset_index()\n",
    "\n",
    "        # Get weekly MAPE\n",
    "        mape_arima = round(abs(arima_df_wk['pv_ia_forecast']/arima_df_wk['pv_ia'] - 1).mean(), 5)\n",
    "        \n",
    "    else:\n",
    "        mape_arima = np.inf\n",
    "    \n",
    "    return mape_arima\n",
    "\n",
    "\n",
    "# Function to get final LGBM forecasts\n",
    "def arima_final(tx_df_fe: pd.DataFrame, boxcox_lambdas_df: pd.DataFrame, tx: str, final_daily_df: pd.DataFrame):\n",
    "    \n",
    "    lmbda = float(boxcox_lambdas_df.loc[boxcox_lambdas_df['taxonomy'] == tx, 'boxcox_lambda'])\n",
    "    \n",
    "    y = tx_df_fe[tx_df_fe['taxonomy'] == tx]['pv_ia_bc']\n",
    "    y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx]['traffic_date']\n",
    "\n",
    "    arima_fit = pm.auto_arima(y,\n",
    "                              max_p = 30,\n",
    "                              max_P = 366,\n",
    "                              error_action='ignore',\n",
    "                              trace=True,\n",
    "                              suppress_warnings=True,\n",
    "                              maxiter=5,\n",
    "                              random_state=120)\n",
    "\n",
    "    y_pred = (arima_fit.predict_in_sample(return_conf_int=True, alpha=0.2))\n",
    "    y_pred = pd.concat([pd.DataFrame(y_pred[0]), pd.DataFrame(y_pred[1])], axis=1)\n",
    "    y_pred.columns = ['page_views_forecast', 'page_views_forecast_lower', 'page_views_forecast_upper']\n",
    "    for col in y_pred.columns:\n",
    "        y_pred[col] = inv_boxcox(y_pred[col], lmbda) - 1\n",
    "    y_pred.index = y.index\n",
    "    y_pred = y_pred.reset_index()\n",
    "    y_pred.index = final_daily_df[(final_daily_df['taxonomy'] == tx) & (final_daily_df['actual_forecast'] == 'actual')].index\n",
    "\n",
    "    y_forecast = (arima_fit.predict(n_periods=182, return_conf_int=True, alpha=0.2))\n",
    "    y_forecast = pd.concat([pd.DataFrame(y_forecast[0]), pd.DataFrame(y_forecast[1])], axis=1)\n",
    "    y_forecast.columns = ['page_views_forecast', 'page_views_forecast_lower', 'page_views_forecast_upper']\n",
    "    for col in y_forecast.columns:\n",
    "        y_forecast[col] = inv_boxcox(y_forecast[col], lmbda) - 1\n",
    "    forecast_dates = pd.Series(pd.date_range(final_daily_df[final_daily_df['actual_forecast'] == 'forecast']['traffic_date'].min(), periods=182))\n",
    "    y_forecast['traffic_date'] = forecast_dates\n",
    "    y_forecast.index = final_daily_df[(final_daily_df['taxonomy'] == tx) & (final_daily_df['actual_forecast'] == 'forecast')].index\n",
    "\n",
    "    final_daily_df.loc[(final_daily_df['taxonomy'] == tx) & (final_daily_df['actual_forecast'] == 'actual'), 'page_views_forecast'] = y_pred['page_views_forecast']\n",
    "    final_daily_df.loc[(final_daily_df['taxonomy'] == tx) & (final_daily_df['actual_forecast'] == 'actual'), 'page_views_forecast_lower'] = y_pred['page_views_forecast_lower']\n",
    "    final_daily_df.loc[(final_daily_df['taxonomy'] == tx) & (final_daily_df['actual_forecast'] == 'actual'), 'page_views_forecast_upper'] = y_pred['page_views_forecast_upper']\n",
    "\n",
    "    final_daily_df.loc[(final_daily_df['taxonomy'] == tx) & (final_daily_df['actual_forecast'] == 'forecast'), 'page_views_forecast'] = y_forecast['page_views_forecast']\n",
    "    final_daily_df.loc[(final_daily_df['taxonomy'] == tx) & (final_daily_df['actual_forecast'] == 'forecast'), 'page_views_forecast_lower'] = y_forecast['page_views_forecast_lower']\n",
    "    final_daily_df.loc[(final_daily_df['taxonomy'] == tx) & (final_daily_df['actual_forecast'] == 'forecast'), 'page_views_forecast_upper'] = y_forecast['page_views_forecast_upper']\n",
    "\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'forecast_model'] = 'ARIMA'\n",
    "\n",
    "    return final_daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2eb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_eval(tx_df_fe: pd.DataFrame, tx: str):\n",
    "\n",
    "    y = tx_df_fe[tx_df_fe['taxonomy'] == tx]['page_views_impute_anomaly']\n",
    "    \n",
    "    if y.shape[0] >= 365:\n",
    "        y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx]['traffic_date']\n",
    "        y = y.reset_index()\n",
    "        y.columns = [\"ds\", \"y\"]\n",
    "\n",
    "        y_train = y.iloc[:-182, :]\n",
    "\n",
    "        prophet_fit = Prophet(daily_seasonality=True)\n",
    "        prophet_fit.fit(y_train)\n",
    "\n",
    "        future = prophet_fit.make_future_dataframe(periods=182)\n",
    "\n",
    "        forecast = prophet_fit.predict(future)\n",
    "\n",
    "        y.columns = ['traffic_date', 'page_views_impute_anomaly']\n",
    "        y['yhat'] = forecast['yhat']\n",
    "        y['train_test'] = 'train'\n",
    "        y.loc[y.index >= y.index.max() - 181, 'train_test'] = 'test'\n",
    "        y['traffic_week'] = y['traffic_date'].dt.to_period('W').dt.start_time\n",
    "\n",
    "        y_wk = y.groupby(['traffic_week', 'train_test'])[['page_views_impute_anomaly', 'yhat']].sum().reset_index()\n",
    "\n",
    "        # Get weekly MAPE\n",
    "        mape_prophet = round(abs(y_wk[y_wk['train_test'] == 'test']['yhat']/y_wk[y_wk['train_test'] == 'test']['page_views_impute_anomaly'] - 1).mean(), 5)\n",
    "    \n",
    "    else:\n",
    "        mape_prophet = np.inf\n",
    "    \n",
    "    return mape_prophet\n",
    "\n",
    "\n",
    "def prophet_final(tx_df_fe: pd.DataFrame, tx: str, final_daily_df: pd.DataFrame):\n",
    "\n",
    "    y = tx_df_fe[tx_df_fe['taxonomy'] == tx]['page_views_impute_anomaly']\n",
    "    y.index = tx_df_fe[tx_df_fe['taxonomy'] == tx]['traffic_date']\n",
    "    y = y.reset_index()\n",
    "    y.columns = [\"ds\", \"y\"]\n",
    "\n",
    "    prophet_fit = Prophet(daily_seasonality=True)\n",
    "    prophet_fit.fit(y)\n",
    "\n",
    "    future = prophet_fit.make_future_dataframe(periods=182)\n",
    "\n",
    "    forecast = prophet_fit.predict(future)[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "    forecast.loc[forecast['yhat_lower'] < 0, 'yhat_lower'] = 0\n",
    "    forecast.index = final_daily_df[(final_daily_df['taxonomy'] == tx)].index\n",
    "\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'page_views_forecast'] = forecast['yhat']\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'page_views_forecast_lower'] = forecast['yhat_lower']\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'page_views_forecast_upper'] = forecast['yhat_upper']\n",
    "\n",
    "    final_daily_df.loc[final_daily_df['taxonomy'] == tx, 'forecast_model'] = 'Prophet'\n",
    "    \n",
    "    return final_daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150075bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tx in tx_df_fe['taxonomy'].unique():\n",
    "    # LightGBM\n",
    "    try:\n",
    "        mape_lgbm = lgbm_eval(tx_df_fe, gbm_features, tx)\n",
    "    except:\n",
    "        mape_lgbm = np.inf\n",
    "        \n",
    "    # Auto ARIMA\n",
    "    try:\n",
    "        mape_arima = arima_eval(tx_df_fe, boxcox_lambdas_df, tx)\n",
    "    except:\n",
    "        mape_arima = np.inf\n",
    "    \n",
    "    # Prophet\n",
    "    try:\n",
    "        mape_prophet = prophet_eval(tx_df_fe, tx)\n",
    "    except:\n",
    "        mape_prophet = np.inf\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(f\"LGBM MAPE for {tx}: {mape_lgbm}\")\n",
    "    print(f\"ARIMA MAPE for {tx}: {mape_arima}\")\n",
    "    print(f\"Prophet MAPE for {tx}: {mape_prophet}\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Final model based on model with lowest MAPE\n",
    "    # try:\n",
    "    if (mape_lgbm <= mape_arima) and (mape_lgbm <= mape_prophet) and (mape_lgbm < np.inf):\n",
    "        final_daily_df = lgbm_final(tx_df_fe, gbm_features, tx, final_daily_df)\n",
    "    elif (mape_arima < mape_lgbm) and (mape_arima <= mape_prophet) and (mape_arima < np.inf):\n",
    "        final_daily_df = arima_final(tx_df_fe, boxcox_lambdas_df, tx, final_daily_df)\n",
    "    elif (mape_prophet < mape_lgbm) and (mape_prophet < mape_arima) and (mape_prophet < np.inf):\n",
    "        final_daily_df = prophet_final(tx_df_fe, tx, final_daily_df)\n",
    "    else:\n",
    "        print(f\"All models failed for {tx}\")\n",
    "    # except:\n",
    "        # print(f\"Failed to get final model and forecasts for {tx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of each taxonomy actual vs. forecast\n",
    "for tx in final_daily_df['taxonomy'].unique():\n",
    "\n",
    "    actual = final_daily_df[(final_daily_df['taxonomy'] == tx) & (final_daily_df[\"actual_forecast\"] == \"actual\")][[\"traffic_date\", \"page_views\"]]\n",
    "    pred = final_daily_df[(final_daily_df['taxonomy'] == tx)  & (final_daily_df[\"actual_forecast\"] == \"actual\")][[\"traffic_date\", \"page_views_forecast\"]]\n",
    "    forecast = final_daily_df[(final_daily_df['taxonomy'] == tx)  & (final_daily_df[\"actual_forecast\"] == \"forecast\")][[\"traffic_date\", \"page_views_forecast\"]]\n",
    "    model = final_daily_df[final_daily_df['taxonomy'] == tx][\"forecast_model\"].unique()[0]\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    ax = plt.axes()\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "    plt.title(f\"Actual and {model} Forecast Page Views for {tx}\")\n",
    "    plt.plot(actual['traffic_date'], actual['page_views'], linewidth=1, color=\"darkblue\")\n",
    "    plt.plot(pred['traffic_date'], pred['page_views_forecast'], linewidth=1, color=\"lightblue\")\n",
    "    plt.plot(forecast['traffic_date'], forecast['page_views_forecast'], linewidth=2, color=\"red\")\n",
    "    plt.xticks(rotation = 45, fontsize=8)\n",
    "    plt.grid(color=\"white\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b551041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often was each model the \"winner\"?\n",
    "display(final_daily_df[['taxonomy', 'forecast_model']].drop_duplicates()['forecast_model'].value_counts())\n",
    "print(\"\")\n",
    "print(final_daily_df[['taxonomy', 'forecast_model']].drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to weekly\n",
    "final_daily_df['upper_pct'] = abs(final_daily_df['page_views_forecast_upper'] - final_daily_df['page_views_forecast'])/final_daily_df['page_views_forecast']\n",
    "final_daily_df['lower_pct'] = abs(final_daily_df['page_views_forecast_lower'] - final_daily_df['page_views_forecast'])/final_daily_df['page_views_forecast']\n",
    "\n",
    "weekly_int_pct = final_daily_df.groupby(['taxonomy', 'week_end_date'])[['lower_pct', 'upper_pct']].max().reset_index().replace(np.inf, 2)\n",
    "\n",
    "final_weekly_df = final_daily_df.groupby(['taxonomy', 'week_end_date', 'forecast_model', 'actual_forecast'])[['page_views', 'page_views_forecast']].sum().reset_index()\n",
    "final_weekly_df = final_weekly_df.merge(weekly_int_pct, on=['taxonomy', 'week_end_date'], how='left')\n",
    "final_weekly_df['page_views_forecast_lower'] = final_weekly_df['page_views_forecast'] * (1 - final_weekly_df['lower_pct'])\n",
    "final_weekly_df['page_views_forecast_upper'] = final_weekly_df['page_views_forecast'] * (1 + final_weekly_df['upper_pct'])\n",
    "final_weekly_df = final_weekly_df.drop(columns=['lower_pct', 'upper_pct'])\n",
    "\n",
    "display(final_weekly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each weekly taxonomy\n",
    "for tx in final_weekly_df['taxonomy'].unique():\n",
    "\n",
    "    actual = final_weekly_df[(final_weekly_df['taxonomy'] == tx) & (final_weekly_df[\"actual_forecast\"] == \"actual\")][[\"week_end_date\", \"page_views\"]]\n",
    "    pred = final_weekly_df[(final_weekly_df['taxonomy'] == tx)  & (final_weekly_df[\"actual_forecast\"] == \"actual\")][[\"week_end_date\", \"page_views_forecast\", \"page_views_forecast_lower\", \"page_views_forecast_upper\"]]\n",
    "    forecast = final_weekly_df[(final_weekly_df['taxonomy'] == tx)  & (final_weekly_df[\"actual_forecast\"] == \"forecast\")][[\"week_end_date\", \"page_views_forecast\", \"page_views_forecast_lower\", \"page_views_forecast_upper\"]]\n",
    "    model = final_weekly_df[final_weekly_df['taxonomy'] == tx][\"forecast_model\"].unique()[0]\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    ax = plt.axes()\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "    plt.title(f\"Weekly Actual and {model} Forecast Page Views for {tx}\")\n",
    "    plt.plot(actual['week_end_date'], actual['page_views'], linewidth=1, color=\"darkblue\")\n",
    "    plt.plot(pred['week_end_date'], pred['page_views_forecast'], linewidth=1, color=\"lightblue\")\n",
    "    plt.plot(forecast['week_end_date'], forecast['page_views_forecast_lower'], linewidth=2, color=\"orange\")\n",
    "    plt.plot(forecast['week_end_date'], forecast['page_views_forecast'], linewidth=2, color=\"red\")\n",
    "    plt.plot(forecast['week_end_date'], forecast['page_views_forecast_upper'], linewidth=2, color=\"orange\")\n",
    "    plt.xticks(rotation = 45, fontsize=8)\n",
    "    plt.grid(color=\"white\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bab710",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weekly_df.to_csv(\"Quant Research Forecasts JH.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b03a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
